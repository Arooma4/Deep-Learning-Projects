{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "a3f87cff66bf4a0caa3a286fdebe9597",
      "df2fa8c66fda402a91598131a09f9d4a",
      "e2f4fc15fa8f4b6186f954a3ad4d1014",
      "2adb0ff33fa04c27930e09c4eb29eb08",
      "80b2305709b244359cba63658cbbd1e6",
      "ef4abadc4bff4c82898b022077affa36",
      "70a6cd23beca4f5cb4e52e563c865d29",
      "28b90276712941339450f37f176fef58",
      "2572853a4a3043a0a9e4e933fa609acc",
      "e879012f22024bbc9b0f977fba965007",
      "0368225dbd70459a9df5bf1744096a4f",
      "55ec74154cad41b1822d356a75327b05",
      "f6ce6f20f2434d179ed75fc86226c84b",
      "7d842a32b4dd443c97fa4ca08a72d32a",
      "f831bd3d950a441f9561a6ca305ac2ce",
      "301e724fd5164b6d8442e15363df0bb8",
      "9386d9645102434482a17965958f25d9",
      "d0ad4fca7f84472bac0c0c45a5ecf6fb",
      "99336bc212474e089b696b8a56292876",
      "3d8625d214584edd82c5597880252682",
      "22a1a9a49c0a4819a9f8eb9e94d46ac4",
      "6bfaf65e7e05482b903168f69bfc4689",
      "48e634319a4e469c8c770bdb5dda0e17",
      "496230b4b5aa4774894210399ca2bc04",
      "53f2d527f3c34a5e9ac0bf5146da3309",
      "d1a820df9b644b4c9b3eeade13a82de5",
      "85d29c3b14fe4385ac60a13a14c6b24a",
      "6e95d1ab7a8a4ad09de05e9c9e15e002",
      "a1e5b089bbcd49edb483ccbcc97943d3",
      "42796e1c12484452a54760ad04d2aab0",
      "fe347fb9f9d9475f8a764fa3ca3f2c83",
      "8fce40177c3f440da25dc56992201da8",
      "c737db7f760441daa610fc482da79aab",
      "d6c9ac03104f4220a64f55661c1ebd66",
      "dd70878977f74354a2085b4d379150c7",
      "d83b0c75dd494902938b58da5efca81a",
      "a1265406e9784047ab9af49a81416082",
      "31dec885cd1b4e05b70426586ec13982",
      "1c6d612a774e42abb319ea225cd49ac6",
      "be9e3d5b6dba4b348a90cbf786bcd270",
      "ae47eab0815742338d5bfcde35c470b5",
      "f6384968e15e438baf5fd612a576d77c",
      "eac3ef563c42440f910e01c2b5e1c0b2",
      "691b101e09c44edf8a836651004d0a78",
      "e12c3a27879c488bab834877eedc2d1e",
      "739018ed908d4ff69d581b3f26d8f08f",
      "734874f917ec43699dc34ee0be6bf39b",
      "0ffa0470fc8a409fb92d7c1f63a9ed23",
      "d6dc4ca14ff343babae09478aa6c901c",
      "3c5ecd4ba381466eb2084f02603a8cbe",
      "b9913109e14e47189b6fcd1b04bb6b10",
      "cf18b7c277f149fbaf572d164bccf32a",
      "a34a145c6751484095a5679793087716",
      "261649fa3403482da854540eff305287",
      "bd01312db333468b810f01e95502d6a1",
      "5dd568721e514ad9aa97530e676da1c7",
      "4e8f03ab3af74fefa5c42cc571ee8ca4",
      "b2de50a16c5b4b7aa13b952486e06655",
      "876899ce25864bc9aa451ce446a5bd82",
      "0b27036d3f3749ccac72e908fab4ae41",
      "93f7e7d999cb407ea1ec4571cf4a9384",
      "08fc54f5ba264b8eb02da11b24597b82",
      "3a236dbc00a94ad08dc29136a6b2f85c",
      "114aaf4a63be430fbf6a809993ea72b6",
      "7e5be03fd26445f89fb8055fe0f11e02",
      "6ea54271739945ea8593acde90c79c34",
      "02dafb7a1c664616a516c62f6aa57fc8",
      "e46d8529b23c41d882707bfaa85721a7",
      "ed2524ac21f2476caea39d6fb4acd59f",
      "65e8feecef164e6d82efddbeff847f36",
      "8712044329db42fda5aec5163a348cb8",
      "46bf94f20ff24659b7c3969cfb2b5e67",
      "6796685713df4187a9aad4c1fa48678b",
      "4ec35f78afc64c9ab5211d91f1956e52",
      "b2319055f1ae4d269f54b3090d024a9a",
      "ad4c369aa5b343638a3c8235cdd3cbbd",
      "9723a75f5465439eba423e221edfb105"
     ]
    },
    "id": "eh0MMtVtZNTF",
    "outputId": "89b1e721-ea5b-4c19-8b22-a605108c5860"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f87cff66bf4a0caa3a286fdebe9597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ec74154cad41b1822d356a75327b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e634319a4e469c8c770bdb5dda0e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c9ac03104f4220a64f55661c1ebd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12c3a27879c488bab834877eedc2d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd568721e514ad9aa97530e676da1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dafb7a1c664616a516c62f6aa57fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: what are you ?\"\n",
      "\n",
      "\"I don't know, but I'm not going to tell you. If you want to find out, you'll have to ask me.\" She turned around and walked back to her room. The door closed behind her, and she turned to look out the window at the night sky. It was a beautiful night, with the stars shining brightly in the sky above. She smiled to herself as she thought about what she would say if she found out what had happened to him. What would she say? Would she tell him that she loved him, that he was the most important person in her life? Or would it be something like, \"I love you so much, Harry, I can't believe you're\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "import torch\n",
    "# Encode input text (the prompt)\n",
    "input_text = \"what are you \"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Define attention mask\n",
    "attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n",
    "# Generate text from the input prompt\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=150,  # Limit the output length\n",
    "    num_beams=5,  # Beam search for better quality output\n",
    "    top_p=0.92,  # Nucleus sampling to control randomness\n",
    "    top_k=50,  # Top-k sampling to limit next token options\n",
    "    do_sample=True,  # Sampling to introduce randomness\n",
    "    repetition_penalty=1.2,  # Prevent repetition in the output\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Ensure padding works with EOS token\n",
    "    attention_mask=attention_mask,  # Set the attention mask\n",
    "    eos_token_id=tokenizer.eos_token_id,  # Ensure stopping at EOS token\n",
    "    no_repeat_ngram_size=2  # Prevent repeating n-grams\n",
    ")\n",
    "\n",
    "# Decode the generated text back to a string\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\",generated_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
