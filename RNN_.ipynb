{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgkAIMdG8Kd80CwbZl/7TU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arooma4/Amazon-clone-page/blob/main/RNN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "11vFknlPyDNR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences #pad sequences helps in reading sequential data\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding ,SimpleRNN , Dense\n",
        "#import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import datasets\n",
        "print(dir(datasets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiQyWdDkzHS3",
        "outputId": "65d5c870-d3e5-47e4-d5bc-553ba8d346d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'boston_housing', 'california_housing', 'cifar10', 'cifar100', 'fashion_mnist', 'imdb', 'mnist', 'reuters']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDB Dataset\n",
        "vocal_size =10000# Only consider the top 10,000 words\n",
        "max_length =500 # Maximum length of each sequence\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=vocal_size)"
      ],
      "metadata": {
        "id": "k1Rdq_65zeP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62ac8e4-18a2-41ce-cd9b-5c6b97de41c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess Data: pad sequences to make them all the same length @scaling\n",
        "x_train=pad_sequences(x_train, maxlen=max_length)\n",
        "x_test=pad_sequences(x_test,maxlen=max_length)"
      ],
      "metadata": {
        "id": "2aI_jmOa1cBj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocal_size ,output_dim=32 ,input_length=max_length), #embedding\n",
        "    SimpleRNN(units=32 ,return_sequences=False), #RNN LAYER\n",
        "    Dense(1,activation='sigmoid') # output layer\n",
        "\n",
        "])\n"
      ],
      "metadata": {
        "id": "rJ693IQm3DC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d80e22-f7c8-423f-83f6-041044bafb75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#complie the model\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Y6_4T7XW36Fd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "print('Training the model...')\n",
        "model.fit(x_train,y_train,epochs=3,batch_size=64,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24OPEqSf4I7Q",
        "outputId": "e1489184-9c58-493f-998d-4dc91114aae8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Epoch 1/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 122ms/step - accuracy: 0.5670 - loss: 0.6800 - val_accuracy: 0.7024 - val_loss: 0.5808\n",
            "Epoch 2/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - accuracy: 0.7656 - loss: 0.5064 - val_accuracy: 0.7886 - val_loss: 0.4659\n",
            "Epoch 3/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 119ms/step - accuracy: 0.8734 - loss: 0.3156 - val_accuracy: 0.7928 - val_loss: 0.4628\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ce28573a810>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "print(\"\\nEvaluating the model\")\n",
        "loss,accuracy=model.evaluate(x_test,y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DMz5CGg79YG",
        "outputId": "bd2a6c55-b2f5-45d2-eda2-a84274685b1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the model\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.7861 - loss: 0.4793\n",
            "Test Accuracy: 79.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test with a custom Input\n",
        "#Decode IMDB word Index\n",
        "word_index=imdb.get_word_index()\n",
        "reverse_word_index={value:key for key, value in word_index.items()}\n",
        "\n",
        "def decode_review(sequence):\n",
        "  return \" \".join([reverse_word_index.get(i-3,\"?\") for i in sequence ])\n",
        "\n",
        "test_review=x_test[0]\n",
        "test_review_padded=pad_sequences([test_review],maxlen=max_length)\n",
        "prediction=model.predict(test_review_padded)\n",
        "print(\"Sentiment Prediction: \",\"Positive\" if prediction > 0.5 else\"Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r882BJ3ZFnzZ",
        "outputId": "4180ed24-40db-40a1-f2d5-cb66c20534f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
            "Sentiment Prediction:  Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1\n",
        "word_index=imdb.get_word_index()\n",
        "reverse_word_index ={value: key for key, value in word_index.items()}\n",
        "\n",
        "#step 2: Preprocess the custom review\n",
        "def preprocess_review(review_text):\n",
        "  #convert the review to lowercase and split into words\n",
        "  words=review_text.lower().split()\n",
        "\n",
        "  #convert words to integers using the imdb word index\n",
        "  tokenized_review = [word_index.get(word,2) for word in words]\n",
        "\n",
        "  #Pad the tokenized sequence\n",
        "  return pad_sequences([tokenized_review],maxlen=max_length)\n",
        "\n",
        "#Step 3: test with custom input\n",
        "custom_review=\"The movie was terrible,with poor acting skills and boring story\"\n",
        "processed_review=preprocess_review(custom_review)\n",
        "\n",
        "#Predict the Sentiment\n",
        "prediction=model.predict(processed_review)\n",
        "print(\"\\nCustom Review:\",custom_review)\n",
        "print(\"Processed Review:\",processed_review)\n",
        "print(\"Sentiment Prediction:\",\"Positive\" if prediction [0][0]>0.5 else \"Negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6cyplVSHoIi",
        "outputId": "8e316a2f-aad1-40cd-832c-d7d558cb2a01"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "\n",
            "Custom Review: The movie was terrible,with poor acting skills and boring story\n",
            "Processed Review: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     1   17   13    2  335  113 1956    2  354   62]]\n",
            "Sentiment Prediction: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Preprocess the custom Review\n",
        "def preprocess_review(review_text):\n",
        "  # Convert the review to lowercase and split into words\n",
        "  words = review_text.lower().split()\n",
        "\n",
        "  # Convert words to integers using the IMDB word index\n",
        "  tokenized_review = [word_index.get(word, 2) for word in words]  # 2 = unknown word\n",
        "\n",
        "  # Pad the tokenized sequence\n",
        "  return pad_sequences([tokenized_review], maxlen=max_length)\n",
        "\n"
      ],
      "metadata": {
        "id": "NbjDu-5qIPJj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_review = \"The movie was terrible and boring\"\n",
        "processed_review = preprocess_review(custom_review)\n",
        "prediction = model.predict(processed_review)\n",
        "\n",
        "print(\"\\nCustom Review:\", custom_review)\n",
        "print(\"Processed Review:\", processed_review)\n",
        "print(\"Sentiment Prediction:\", \"Positive\" if prediction[0][0] > 0.5 else \"Negative\")\n"
      ],
      "metadata": {
        "id": "_tun9IAlJdgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a15b21c-9552-4415-feee-13debfdee8c2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\n",
            "Custom Review: The movie was terrible and boring\n",
            "Processed Review: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   1  17  13 391   2 354]]\n",
            "Sentiment Prediction: Positive\n"
          ]
        }
      ]
    }
  ]
}